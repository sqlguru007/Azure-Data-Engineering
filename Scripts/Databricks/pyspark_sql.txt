# Databricks notebook source
# DBTITLE 1,mounting adls location
dbutils.fs.mount(
  source = "wasbs://CONTAINER@STORAGENAME.blob.core.windows.net",
  mount_point = "/mnt/retail",
  extra_configs = {"fs.azure.account.key.STORAGENAME.blob.core.windows.net":"PASSYORACCESSKEY"})



# COMMAND ----------

dbutils.fs.ls('/mnt/retail/')

# COMMAND ----------

dbutils.fs.ls('/mnt/retail/bronze')

# COMMAND ----------

from pyspark.sql.functions import col, upper, trim, when
# Step 1: Read raw JSON (bronze)
bronze_df = spark.read.parquet("/mnt/retail/bronze")
display(bronze_df)


# COMMAND ----------

df_clean1 = bronze_df.filter(
    (col("TransactionID").isNotNull()) &
    (col("CustomerID").isNotNull()) &
    (col("TransactionDate").isNotNull())
)
display(df_clean1)

# COMMAND ----------

# Step 3: Trim and standardize text fields
df_clean2 = df_clean1.withColumn("PaymentType", upper(trim(col("PaymentType")))) \
                     .withColumn("StoreRegion", upper(trim(col("StoreRegion")))) \
                     .withColumn("DeviceUsed", upper(trim(col("DeviceUsed"))))

display(df_clean2)

# COMMAND ----------

df_clean3 = df_clean2.withColumn("TransactionDate", col("TransactionDate").cast("timestamp")) \
                     .withColumn("Quantity", col("Quantity").cast("int")) \
                     .withColumn("Amount", col("Amount").cast("float")) \
                     .withColumn("Discount", col("Discount").cast("float"))
display(df_clean3)

# COMMAND ----------

# Step 5: Filter out invalid quantity, negative values, excessive discounts
df_clean4 = df_clean3.filter(
    (col("Quantity") > 0) & 
    (col("Amount") > 0) &
    (col("Discount") <= col("Amount")))
display(df_clean4)

# COMMAND ----------

df_clean5 = df_clean4.dropDuplicates(["TransactionID"])

# COMMAND ----------

# DBTITLE 1,silver dataset
# Step 2: Drop critical nulls
df_clean1 = bronze_df.filter(
    (col("TransactionID").isNotNull()) &
    (col("CustomerID").isNotNull()) &
    (col("TransactionDate").isNotNull())
)

# Step 3: Trim and standardize text fields
df_clean2 = df_clean1.withColumn("PaymentType", upper(trim(col("PaymentType")))) \
                     .withColumn("StoreRegion", upper(trim(col("StoreRegion")))) \
                     .withColumn("DeviceUsed", upper(trim(col("DeviceUsed"))))

# Step 4: Cast types
df_clean3 = df_clean2.withColumn("TransactionDate", col("TransactionDate").cast("timestamp")) \
                     .withColumn("Quantity", col("Quantity").cast("int")) \
                     .withColumn("Amount", col("Amount").cast("float")) \
                     .withColumn("Discount", col("Discount").cast("float"))

# Step 5: Filter out invalid quantity, negative values, excessive discounts
df_clean4 = df_clean3.filter(
    (col("Quantity") > 0) & 
    (col("Amount") > 0) &
    (col("Discount") <= col("Amount"))
)

# Step 6: Drop duplicates
df_clean5 = df_clean4.dropDuplicates(["TransactionID"])

# Step 7: Write to Silver
df_clean5.write.format("parquet").mode("overwrite").save("/mnt/retail/silver")


# COMMAND ----------

silver_df=spark.read.parquet('/mnt/retail/silver/')
display(silver_df)

# COMMAND ----------

# DBTITLE 1,gold dataset
silver_df.createOrReplaceTempView('retail_data')

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from retail_data

# COMMAND ----------

# DBTITLE 1,daily revenue by purchase
# MAGIC %sql
# MAGIC select date(TransactionDate),sum(amount) total_revenue,count(distinct TransactionID) total_purchase from retail_data
# MAGIC group by 1

# COMMAND ----------

# DBTITLE 1,REVENUE BY PAYMENT TYPE
# MAGIC %sql
# MAGIC select sum(amount) total_revenue,PAYMENTTYPE from retail_data
# MAGIC group by 2

# COMMAND ----------

# DBTITLE 1,STORE PERFORMANCE
# MAGIC %sql
# MAGIC select sum(amount) total_revenue,STORELOCATION from retail_data
# MAGIC group by 2

# COMMAND ----------

# DBTITLE 1,LOYALITY LEVEL REVENUE CONTRIBUTION
# MAGIC %sql
# MAGIC select sum(amount) total_revenue,CustomerLoyaltyLevel
# MAGIC  from retail_data
# MAGIC group by 2

# COMMAND ----------

# DBTITLE 1,PRODUCT CATEGORY SALES
# MAGIC %sql
# MAGIC select sum(amount) total_revenue,ProductCategory
# MAGIC
# MAGIC  from retail_data
# MAGIC group by 2

# COMMAND ----------

