# Databricks notebook source

configs = {"fs.azure.account.auth.type": "OAuth",
          "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
          "fs.azure.account.oauth2.client.id": "<application-id>",
          "fs.azure.account.oauth2.client.secret": dbutils.secrets.get(scope="<scope-name>",key="<service-credential-key-name>"),
          "fs.azure.account.oauth2.client.endpoint": "https://login.microsoftonline.com/<directory-id>/oauth2/token"}

# Optionally, you can add <directory-name> to the source URI of your mount point.
dbutils.fs.mount(
  source = "abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/",
  mount_point = "/mnt/storagename/container",
  extra_configs = configs)

# COMMAND ----------

dbutils.fs.ls('/mnt/storagename/')

# COMMAND ----------

dbutils.fs.ls('/mnt/storagename/bronze')

# COMMAND ----------

from pyspark.sql.functions import col, upper, trim, when
# Step 1: Read raw JSON (bronze)
bronze_df = spark.read.parquet("/mnt/storagename/bronze")
display(bronze_df)


# COMMAND ----------

df_clean1 = bronze_df.filter(
    (col("TransactionID").isNotNull()) &
    (col("CustomerID").isNotNull()) &
    (col("TransactionDate").isNotNull())
)
display(df_clean1)

# COMMAND ----------

# Step 3: Trim and standardize text fields
df_clean2 = df_clean1.withColumn("PaymentType", upper(trim(col("PaymentType")))) \
                     .withColumn("StoreRegion", upper(trim(col("StoreRegion")))) \
                     .withColumn("DeviceUsed", upper(trim(col("DeviceUsed"))))

display(df_clean2)

# COMMAND ----------

df_clean3 = df_clean2.withColumn("TransactionDate", col("TransactionDate").cast("timestamp")) \
                     .withColumn("Quantity", col("Quantity").cast("int")) \
                     .withColumn("Amount", col("Amount").cast("float")) \
                     .withColumn("Discount", col("Discount").cast("float"))
display(df_clean3)

# COMMAND ----------

# Step 5: Filter out invalid quantity, negative values, excessive discounts
df_clean4 = df_clean3.filter(
    (col("Quantity") > 0) & 
    (col("Amount") > 0) &
    (col("Discount") <= col("Amount")))
display(df_clean4)

# COMMAND ----------

df_clean5 = df_clean4.dropDuplicates(["TransactionID"])

# COMMAND ----------

# DBTITLE 1,silver dataset
# Step 2: Drop critical nulls
df_clean1 = bronze_df.filter(
    (col("TransactionID").isNotNull()) &
    (col("CustomerID").isNotNull()) &
    (col("TransactionDate").isNotNull())
)

# Step 3: Trim and standardize text fields
df_clean2 = df_clean1.withColumn("PaymentType", upper(trim(col("PaymentType")))) \
                     .withColumn("StoreRegion", upper(trim(col("StoreRegion")))) \
                     .withColumn("DeviceUsed", upper(trim(col("DeviceUsed"))))

# Step 4: Cast types
df_clean3 = df_clean2.withColumn("TransactionDate", col("TransactionDate").cast("timestamp")) \
                     .withColumn("Quantity", col("Quantity").cast("int")) \
                     .withColumn("Amount", col("Amount").cast("float")) \
                     .withColumn("Discount", col("Discount").cast("float"))

# Step 5: Filter out invalid quantity, negative values, excessive discounts
df_clean4 = df_clean3.filter(
    (col("Quantity") > 0) & 
    (col("Amount") > 0) &
    (col("Discount") <= col("Amount"))
)

# Step 6: Drop duplicates
df_clean5 = df_clean4.dropDuplicates(["TransactionID"])

# Step 7: Write to Silver
df_clean5.write.format("parquet").mode("overwrite").save("/mnt/storagename/silver")


# COMMAND ----------

silver_df=spark.read.parquet('/mnt/storagename/silver/')
display(silver_df)

# COMMAND ----------

# DBTITLE 1,gold dataset
silver_df.createOrReplaceTempView('retail_data')

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from retail_data

# COMMAND ----------

# DBTITLE 1,daily revenue by purchase
# MAGIC %sql
# MAGIC select date(TransactionDate),sum(amount) total_revenue,count(distinct TransactionID) total_purchase from retail_data
# MAGIC group by 1

# COMMAND ----------

# DBTITLE 1,REVENUE BY PAYMENT TYPE
# MAGIC %sql
# MAGIC select sum(amount) total_revenue,PAYMENTTYPE from retail_data
# MAGIC group by 2

# COMMAND ----------

# DBTITLE 1,STORE PERFORMANCE
# MAGIC %sql
# MAGIC select sum(amount) total_revenue,STORELOCATION from retail_data
# MAGIC group by 2

# COMMAND ----------

# DBTITLE 1,LOYALITY LEVEL REVENUE CONTRIBUTION
# MAGIC %sql
# MAGIC select sum(amount) total_revenue,CustomerLoyaltyLevel
# MAGIC  from retail_data
# MAGIC group by 2

# COMMAND ----------

# DBTITLE 1,PRODUCT CATEGORY SALES
# MAGIC %sql
# MAGIC select sum(amount) total_revenue,ProductCategory
# MAGIC
# MAGIC  from retail_data
# MAGIC group by 2

# COMMAND ----------

