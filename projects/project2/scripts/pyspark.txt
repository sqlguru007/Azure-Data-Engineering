# STEP 1: MOUNT YOUR STORAGE




# STEP2: LIST THE FILES

dbutils.fs.ls('/mnt/store_project/bronze/transaction/')

# STEP 3: CREATE THE DATAFRAMES

# Read the bronze layer
# Read raw data from Bronze layer
df_transactions = spark.read.parquet('/mnt/store_project/bronze/transaction/')
df_products = spark.read.parquet('/mnt/store_project/bronze/product/')
df_shop = spark.read.parquet('/mnt/store_project/bronze/shop/') 

df_customers = spark.read.parquet('/mnt/store_project/bronze/customer/')
display(df_customers)




# STEP 4: CHECK THE DATAFRAME

display(df_transactions)

# STEP 5: START CLEANING

# Create silver layer - data cleaning

from pyspark.sql.functions import col

# Convert types and clean data
df_transactions = df_transactions.select(
    col("transaction_id").cast("int"),
    col("customer_id").cast("int"),
    col("product_id").cast("int"),
    col("store_id").cast("int"),
    col("quantity").cast("int"),
    col("transaction_date").cast("date")
)

df_products = df_products.select(
    col("product_id").cast("int"),
    col("product_name"),
    col("category"),
    col("price").cast("double")
)

df_shop = df_shop.select(
    col("store_id").cast("int"),
    col("store_name"),
    col("location")
)

df_customers = df_customers.select(
    "customer_id", "first_name", "last_name", "email", "city", "registration_date"
).dropDuplicates(["customer_id"])


# STEP 6: JOIN THE DATAFRAMES

# Join all data together

df_silver = df_transactions \
    .join(df_customers, "customer_id") \
    .join(df_products, "product_id") \
    .join(df_shop, "store_id") \
    .withColumn("total_amount", col("quantity") * col("price"))


# VALIDATE THE SILVER DATAFRAMES

display(df_silver)

# STEP 7: COPY TO THE SILVER CONTAINER

# Copy to adls locatioN
silver_path = "/mnt/store_project/silver/"

df_silver.write.mode("overwrite").format("delta").save(silver_path)


# CREATE A DELTA TABLE FOR THE SILVER DATASET

# Create silver dataset

spark.sql(f"""
CREATE TABLE store_silver_cleaned
USING DELTA
LOCATION '/mnt/store_project/silver/'
""")


# STEP 8: Use SQL to validate the data

# MAGIC %sql 
select * from store_silver_cleaned

# STEP 9: CREATE A NEW DATAFRAME IN PREPARATION FOR THE GOLD LAYER


# Load cleaned transactions from Silver layer
silver_df = spark.read.format("delta").load("/mnt/store_project/silver/")


# VALIDATE THE NEW DATAFRAME

display(silver_df)

# CREATE THE GOLD DATAFRAME BY AGGREEGATING THE SILVER DATAFRAME ACCORDING TO THE REQUIREMENT

from pyspark.sql.functions import sum, countDistinct, avg

gold_df = silver_df.groupBy(
    "transaction_date",
    "product_id", "product_name", "category",
    "store_id", "store_name", "location"
).agg(
    sum("quantity").alias("total_quantity_sold"),
    sum("total_amount").alias("total_sales_amount"),
    countDistinct("transaction_id").alias("number_of_transactions"),
    avg("total_amount").alias("average_transaction_value")
)


# VALIDATE THE GOLD DATAFRAME

display(gold_df)


gold_path = "/mnt/store_project/gold/"

gold_df.write.mode("overwrite").format("delta").save(gold_path)


# CREATE A DATABRICKS TABLE USING SPARK SQL

spark.sql("""
CREATE TABLE store_gold_sales_summary
USING DELTA
LOCATION '/mnt/store_project/gold/' """)


# VALIDATE THE DATA USING SQL

# MAGIC 
%sql 
select * from store_gold_sales_summary



